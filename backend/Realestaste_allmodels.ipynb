{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9c7d4944",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c7d4944",
        "outputId": "3667419b-96a8-4764-de8f-eea97d7ed2ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Information:\n",
            "Shape: (13320, 8)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13320 entries, 0 to 13319\n",
            "Data columns (total 8 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   area_type   13320 non-null  object \n",
            " 1   location    13319 non-null  object \n",
            " 2   size        13304 non-null  object \n",
            " 3   society     7818 non-null   object \n",
            " 4   total_sqft  13320 non-null  object \n",
            " 5   bath        13247 non-null  float64\n",
            " 6   balcony     12711 non-null  float64\n",
            " 7   price       13320 non-null  float64\n",
            "dtypes: float64(3), object(5)\n",
            "memory usage: 832.6+ KB\n",
            "\n",
            "Missing Values:\n",
            "area_type        0\n",
            "location         1\n",
            "size            16\n",
            "society       5502\n",
            "total_sqft       0\n",
            "bath            73\n",
            "balcony        609\n",
            "price            0\n",
            "dtype: int64\n",
            "\n",
            "---- Data Cleaning ----\n",
            "Shape after dropping nulls: (7496, 8)\n",
            "\n",
            "---- Removing Outliers ----\n",
            "Removed entries where bath > bhk + 2: (7479, 10)\n",
            "Removed 6 outliers from price_per_sqft\n",
            "Removed entries with total_sqft/bhk < 300: (7433, 10)\n",
            "\n",
            "---- Feature Engineering ----\n",
            "\n",
            "---- Exploratory Data Analysis ----\n",
            "\n",
            "---- Preparing Data for Modeling ----\n",
            "Training set size: (5946, 6)\n",
            "Test set size: (1487, 6)\n",
            "\n",
            "---- Model Training and Evaluation ----\n",
            "\n",
            "Model: Linear Regression\n",
            "RMSE: 91.15\n",
            "MAE: 26.27\n",
            "R²: 0.3698\n",
            "\n",
            "Model: Random Forest\n",
            "RMSE: 63.85\n",
            "MAE: 19.82\n",
            "R²: 0.6908\n",
            "\n",
            "Model: Gradient Boosting\n",
            "RMSE: 63.21\n",
            "MAE: 22.27\n",
            "R²: 0.6970\n",
            "\n",
            "Model: XGBoost\n",
            "RMSE: 71.94\n",
            "MAE: 19.53\n",
            "R²: 0.6074\n",
            "\n",
            "Best Model: Gradient Boosting\n",
            "R² Score: 0.6970\n",
            "\n",
            "---- Hyperparameter Tuning ----\n",
            "Best parameters: {'regressor__learning_rate': 0.2, 'regressor__max_depth': 3, 'regressor__n_estimators': 200}\n",
            "Tuned Model Performance:\n",
            "RMSE: 70.61\n",
            "MAE: 21.43\n",
            "R²: 0.6219\n",
            "\n",
            "---- Training Deep Learning Model ----\n",
            "Epoch 1/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 19221.5273 - mae: 98.7037 - val_loss: 17606.4434 - val_mae: 90.5037\n",
            "Epoch 2/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 14762.6709 - mae: 84.4255 - val_loss: 12183.9229 - val_mae: 78.0784\n",
            "Epoch 3/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6898.9312 - mae: 59.3860 - val_loss: 6178.2393 - val_mae: 51.0274\n",
            "Epoch 4/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5015.2832 - mae: 46.9039 - val_loss: 3789.5571 - val_mae: 34.9038\n",
            "Epoch 5/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3364.9521 - mae: 39.2307 - val_loss: 2888.4670 - val_mae: 28.9705\n",
            "Epoch 6/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4099.9917 - mae: 35.1195 - val_loss: 2522.8767 - val_mae: 27.6431\n",
            "Epoch 7/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4174.2422 - mae: 34.6668 - val_loss: 2389.6404 - val_mae: 25.7223\n",
            "Epoch 8/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2828.3364 - mae: 31.6041 - val_loss: 2306.4668 - val_mae: 24.1569\n",
            "Epoch 9/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2650.4121 - mae: 30.9827 - val_loss: 2214.8855 - val_mae: 23.0651\n",
            "Epoch 10/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3694.3911 - mae: 30.3461 - val_loss: 1931.7156 - val_mae: 23.0264\n",
            "Epoch 11/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2663.5322 - mae: 30.3698 - val_loss: 2365.3096 - val_mae: 23.0155\n",
            "Epoch 12/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2706.4487 - mae: 29.5628 - val_loss: 2086.5505 - val_mae: 22.0954\n",
            "Epoch 13/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2756.4341 - mae: 30.0701 - val_loss: 1926.5170 - val_mae: 21.6196\n",
            "Epoch 14/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2570.3115 - mae: 28.6588 - val_loss: 1818.4243 - val_mae: 21.2379\n",
            "Epoch 15/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3158.9138 - mae: 30.5259 - val_loss: 1954.4236 - val_mae: 21.4668\n",
            "Epoch 16/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3349.4951 - mae: 29.0422 - val_loss: 1885.3257 - val_mae: 21.3941\n",
            "Epoch 17/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3558.6475 - mae: 29.9911 - val_loss: 2066.6704 - val_mae: 21.5406\n",
            "Epoch 18/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2759.0847 - mae: 27.6944 - val_loss: 1862.8616 - val_mae: 21.5553\n",
            "Epoch 19/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2948.2461 - mae: 29.1208 - val_loss: 1862.5853 - val_mae: 21.0520\n",
            "Epoch 20/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2143.9285 - mae: 27.5110 - val_loss: 1864.4012 - val_mae: 21.2423\n",
            "Epoch 21/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2457.6091 - mae: 28.2139 - val_loss: 1840.2963 - val_mae: 21.9979\n",
            "Epoch 22/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2423.9482 - mae: 28.0863 - val_loss: 1973.9225 - val_mae: 21.2447\n",
            "Epoch 23/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1879.3702 - mae: 26.9559 - val_loss: 2092.8896 - val_mae: 21.4916\n",
            "Epoch 24/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2470.0266 - mae: 27.7202 - val_loss: 2039.9572 - val_mae: 20.8222\n",
            "Epoch 25/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2041.9590 - mae: 27.1030 - val_loss: 2096.1929 - val_mae: 21.3629\n",
            "Epoch 26/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2188.3728 - mae: 28.1549 - val_loss: 1839.5378 - val_mae: 20.9349\n",
            "Epoch 27/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2060.0859 - mae: 26.3646 - val_loss: 1906.0481 - val_mae: 21.0164\n",
            "Epoch 28/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2742.3684 - mae: 28.6022 - val_loss: 1787.2556 - val_mae: 20.6881\n",
            "Epoch 29/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2277.4116 - mae: 27.3848 - val_loss: 1852.7914 - val_mae: 21.0017\n",
            "Epoch 30/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2642.6489 - mae: 27.7680 - val_loss: 1704.0365 - val_mae: 20.7347\n",
            "Epoch 31/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2225.4434 - mae: 27.9209 - val_loss: 1849.2372 - val_mae: 20.9296\n",
            "Epoch 32/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2111.9712 - mae: 26.7644 - val_loss: 1745.7455 - val_mae: 20.6790\n",
            "Epoch 33/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2233.1350 - mae: 26.8776 - val_loss: 1804.8735 - val_mae: 20.3951\n",
            "Epoch 34/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2254.6287 - mae: 27.8197 - val_loss: 1816.5088 - val_mae: 20.5053\n",
            "Epoch 35/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2226.7397 - mae: 27.3526 - val_loss: 1814.0248 - val_mae: 21.1598\n",
            "Epoch 36/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2256.7634 - mae: 27.5269 - val_loss: 1826.9551 - val_mae: 20.9348\n",
            "Epoch 37/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3046.1121 - mae: 27.6606 - val_loss: 1785.4027 - val_mae: 20.8441\n",
            "Epoch 38/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1975.6327 - mae: 26.1250 - val_loss: 1761.5973 - val_mae: 20.5538\n",
            "Epoch 39/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1979.8705 - mae: 25.6944 - val_loss: 1917.5892 - val_mae: 20.9857\n",
            "Epoch 40/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2293.7107 - mae: 25.9546 - val_loss: 1854.7360 - val_mae: 20.5853\n",
            "Epoch 41/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2009.6646 - mae: 26.3212 - val_loss: 1806.2812 - val_mae: 20.6029\n",
            "Epoch 42/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2074.2837 - mae: 26.4220 - val_loss: 1754.9860 - val_mae: 20.6068\n",
            "Epoch 43/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2165.9316 - mae: 26.1574 - val_loss: 1713.3617 - val_mae: 20.5137\n",
            "Epoch 44/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3137.4861 - mae: 26.4624 - val_loss: 1676.5986 - val_mae: 20.3925\n",
            "Epoch 45/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2169.2290 - mae: 25.5524 - val_loss: 1729.6860 - val_mae: 20.3268\n",
            "Epoch 46/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1919.0927 - mae: 25.7024 - val_loss: 1729.8115 - val_mae: 20.1135\n",
            "Epoch 47/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2840.2644 - mae: 27.4929 - val_loss: 1740.3247 - val_mae: 20.3192\n",
            "Epoch 48/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2389.2961 - mae: 26.7807 - val_loss: 1754.4036 - val_mae: 20.4493\n",
            "Epoch 49/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2268.7029 - mae: 26.1784 - val_loss: 1841.9817 - val_mae: 20.6799\n",
            "Epoch 50/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1951.1799 - mae: 25.0836 - val_loss: 1766.6376 - val_mae: 20.4682\n",
            "Epoch 51/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2596.6904 - mae: 25.2133 - val_loss: 1636.8647 - val_mae: 20.1510\n",
            "Epoch 52/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2921.5195 - mae: 25.0896 - val_loss: 1726.6235 - val_mae: 20.4852\n",
            "Epoch 53/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1908.4590 - mae: 25.4526 - val_loss: 1845.9138 - val_mae: 20.7351\n",
            "Epoch 54/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1848.2285 - mae: 25.3632 - val_loss: 1798.1093 - val_mae: 21.1527\n",
            "Epoch 55/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2646.4187 - mae: 25.7215 - val_loss: 1663.1072 - val_mae: 20.2369\n",
            "Epoch 56/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2079.3430 - mae: 25.6598 - val_loss: 1709.9989 - val_mae: 20.7260\n",
            "Epoch 57/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1982.5750 - mae: 25.3311 - val_loss: 1724.8414 - val_mae: 21.0587\n",
            "Epoch 58/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1964.6982 - mae: 24.6825 - val_loss: 1720.0305 - val_mae: 20.9678\n",
            "Epoch 59/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1911.6798 - mae: 24.6738 - val_loss: 1795.2626 - val_mae: 20.7825\n",
            "Epoch 60/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2422.1421 - mae: 26.0172 - val_loss: 1730.5496 - val_mae: 20.6205\n",
            "Epoch 61/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2522.9814 - mae: 24.6633 - val_loss: 1665.2468 - val_mae: 20.5148\n",
            "Epoch 62/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2020.2631 - mae: 25.0624 - val_loss: 1682.3247 - val_mae: 20.6665\n",
            "Epoch 63/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2110.4521 - mae: 25.6319 - val_loss: 1620.6501 - val_mae: 20.1768\n",
            "Epoch 64/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1847.0707 - mae: 23.4545 - val_loss: 1753.1958 - val_mae: 20.4531\n",
            "Epoch 65/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1967.5105 - mae: 24.0283 - val_loss: 1769.6824 - val_mae: 20.1098\n",
            "Epoch 66/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1745.7764 - mae: 24.1488 - val_loss: 1690.9249 - val_mae: 20.1348\n",
            "Epoch 67/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1710.3282 - mae: 24.4064 - val_loss: 1774.2809 - val_mae: 20.3769\n",
            "Epoch 68/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2694.9807 - mae: 25.3000 - val_loss: 1666.2831 - val_mae: 20.3657\n",
            "Epoch 69/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1937.4685 - mae: 24.9897 - val_loss: 1652.1992 - val_mae: 20.3288\n",
            "Epoch 70/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3188.0310 - mae: 25.7970 - val_loss: 1634.8851 - val_mae: 20.0883\n",
            "Epoch 71/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1731.0175 - mae: 24.1524 - val_loss: 1680.5027 - val_mae: 20.1975\n",
            "Epoch 72/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1648.5924 - mae: 24.1835 - val_loss: 1702.1449 - val_mae: 20.4572\n",
            "Epoch 73/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2166.9651 - mae: 24.7866 - val_loss: 1787.5341 - val_mae: 20.2289\n",
            "Epoch 74/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1853.9236 - mae: 24.0002 - val_loss: 1719.0338 - val_mae: 20.3763\n",
            "Epoch 75/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1784.6797 - mae: 23.9543 - val_loss: 1752.8024 - val_mae: 20.1910\n",
            "Epoch 76/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2086.6238 - mae: 24.0201 - val_loss: 1742.5404 - val_mae: 20.4058\n",
            "Epoch 77/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2127.3262 - mae: 23.9478 - val_loss: 1855.8196 - val_mae: 20.3419\n",
            "Epoch 78/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1971.3666 - mae: 24.5994 - val_loss: 1863.3109 - val_mae: 20.4533\n",
            "Epoch 79/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2416.5703 - mae: 25.2548 - val_loss: 1690.3969 - val_mae: 19.8328\n",
            "Epoch 80/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2099.6597 - mae: 24.7220 - val_loss: 1816.3447 - val_mae: 20.7919\n",
            "Epoch 81/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1824.8444 - mae: 23.7246 - val_loss: 1733.6573 - val_mae: 20.2345\n",
            "Epoch 82/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1553.0504 - mae: 22.6606 - val_loss: 1668.3000 - val_mae: 20.1299\n",
            "Epoch 83/100\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1792.2750 - mae: 23.2396 - val_loss: 1775.5317 - val_mae: 20.3407\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "\n",
            "Deep Learning Model Performance:\n",
            "RMSE: 186.34\n",
            "MAE: 26.23\n",
            "R²: -1.6339\n",
            "Traditional ML model (Gradient Boosting) outperforms Deep Learning.\n",
            "\n",
            "---- Saving the Best Model ----\n",
            "Model saved as 'model.pkl'\n",
            "Preprocessor saved as 'preprocessor.pkl'\n",
            "Model metadata saved as 'model_metadata.json'\n",
            "\n",
            "---- Model Building Complete ----\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "import xgboost as XGBRegressor\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/Bengaluru_House_Data.csv')\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "df.info()\n",
        "df.head()\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Data Cleaning and Preprocessing\n",
        "print(\"\\n---- Data Cleaning ----\")\n",
        "\n",
        "# Handle missing values\n",
        "df.dropna(inplace=True)\n",
        "print(f\"Shape after dropping nulls: {df.shape}\")\n",
        "\n",
        "# Convert size to numeric (assuming it's in format like '2 BHK', extracting the number)\n",
        "df['bhk'] = df['size'].apply(lambda x: int(x.split()[0]) if isinstance(x, str) and len(x.split()) > 0 and x.split()[0].isdigit() else np.nan)\n",
        "df.dropna(subset=['bhk'], inplace=True)\n",
        "df['bhk'] = df['bhk'].astype(int)\n",
        "\n",
        "# Extract total_sqft as numeric\n",
        "def convert_sqft_to_num(x):\n",
        "    tokens = x.split('-')\n",
        "    if len(tokens) == 2:\n",
        "        return (float(tokens[0]) + float(tokens[1]))/2\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "df['total_sqft'] = df['total_sqft'].apply(convert_sqft_to_num)\n",
        "df.dropna(subset=['total_sqft'], inplace=True)\n",
        "\n",
        "# Create price per square feet\n",
        "df['price_per_sqft'] = df['price'] * 100000 / df['total_sqft']\n",
        "\n",
        "# Handle bath\n",
        "df.dropna(subset=['bath'], inplace=True)\n",
        "df['bath'] = df['bath'].astype(int)\n",
        "\n",
        "# Remove outliers\n",
        "print(\"\\n---- Removing Outliers ----\")\n",
        "\n",
        "# Function to remove outliers based on standard deviation\n",
        "def remove_outliers_std(df, column, n_std):\n",
        "    mean = df[column].mean()\n",
        "    std = df[column].std()\n",
        "    df_out = df[(df[column] <= mean + (n_std * std)) & (df[column] >= mean - (n_std * std))]\n",
        "    print(f\"Removed {df.shape[0] - df_out.shape[0]} outliers from {column}\")\n",
        "    return df_out\n",
        "\n",
        "# Remove entries where number of bathrooms > number of bedrooms + 2\n",
        "df = df[df['bath'] <= df['bhk'] + 2]\n",
        "print(f\"Removed entries where bath > bhk + 2: {df.shape}\")\n",
        "\n",
        "# Remove outliers based on price_per_sqft\n",
        "df = remove_outliers_std(df, 'price_per_sqft', 3)\n",
        "\n",
        "# Filter out properties with too small area per bedroom\n",
        "df = df[df['total_sqft']/df['bhk'] >= 300]\n",
        "print(f\"Removed entries with total_sqft/bhk < 300: {df.shape}\")\n",
        "\n",
        "# Feature Engineering\n",
        "print(\"\\n---- Feature Engineering ----\")\n",
        "\n",
        "# Create a new feature for price per bedroom\n",
        "df['price_per_bedroom'] = df['price'] / df['bhk']\n",
        "\n",
        "# Convert balcony to numeric\n",
        "df['balcony'] = pd.to_numeric(df['balcony'], errors='coerce')\n",
        "df.dropna(subset=['balcony'], inplace=True)\n",
        "df['balcony'] = df['balcony'].astype(int)\n",
        "\n",
        "# Exploratory Data Analysis\n",
        "print(\"\\n---- Exploratory Data Analysis ----\")\n",
        "\n",
        "\n",
        "\n",
        "# Distribution of prices\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['price'], kde=True)\n",
        "plt.title('Price Distribution')\n",
        "plt.xlabel('Price (Lakhs)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.savefig('price_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# Location vs Price\n",
        "plt.figure(figsize=(16, 8))\n",
        "locations = df.groupby('location')['price'].mean().sort_values(ascending=False).head(15).index\n",
        "location_price = df[df['location'].isin(locations)].groupby('location')['price'].mean().sort_values(ascending=False)\n",
        "sns.barplot(x=location_price.index, y=location_price.values)\n",
        "plt.title('Top 15 Locations by Average Price')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('location_vs_price.png')\n",
        "plt.close()\n",
        "\n",
        "# Price vs Area type\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='area_type', y='price', data=df)\n",
        "plt.title('Price vs Area Type')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('area_type_vs_price.png')\n",
        "plt.close()\n",
        "\n",
        "# Prepare data for modeling\n",
        "print(\"\\n---- Preparing Data for Modeling ----\")\n",
        "\n",
        "# Select features to use\n",
        "X = df[['area_type', 'location', 'bhk', 'total_sqft', 'bath', 'balcony']]\n",
        "y = df['price']\n",
        "\n",
        "# Count unique locations\n",
        "location_counts = X['location'].value_counts()\n",
        "# Group less frequent locations into 'Other'\n",
        "threshold = 10  # Minimum number of properties to be a distinct location\n",
        "X['location'] = X['location'].apply(lambda x: 'Other' if location_counts[x] < threshold else x)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n",
        "\n",
        "# Feature preprocessing\n",
        "numeric_features = ['bhk', 'total_sqft', 'bath', 'balcony']\n",
        "categorical_features = ['area_type', 'location']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Model Training and Evaluation\n",
        "print(\"\\n---- Model Training and Evaluation ----\")\n",
        "\n",
        "# Create pipelines for different models\n",
        "pipeline_lr = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "pipeline_rf = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "pipeline_gb = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', GradientBoostingRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "pipeline_xgb = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', XGBRegressor.XGBRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# Function to evaluate models\n",
        "def evaluate_model(model, name, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"R²: {r2:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'name': name,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'r2': r2\n",
        "    }\n",
        "\n",
        "# Train and evaluate each model\n",
        "models = [\n",
        "    (pipeline_lr, \"Linear Regression\"),\n",
        "    (pipeline_rf, \"Random Forest\"),\n",
        "    (pipeline_gb, \"Gradient Boosting\"),\n",
        "    (pipeline_xgb, \"XGBoost\")\n",
        "]\n",
        "\n",
        "results = []\n",
        "for model, name in models:\n",
        "    result = evaluate_model(model, name, X_train, X_test, y_train, y_test)\n",
        "    results.append(result)\n",
        "\n",
        "# Sort results by R²\n",
        "results.sort(key=lambda x: x['r2'], reverse=True)\n",
        "best_model = results[0]['model']\n",
        "best_model_name = results[0]['name']\n",
        "\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "print(f\"R² Score: {results[0]['r2']:.4f}\")\n",
        "\n",
        "# Hyperparameter tuning for the best model\n",
        "print(\"\\n---- Hyperparameter Tuning ----\")\n",
        "\n",
        "if best_model_name == \"Random Forest\":\n",
        "    param_grid = {\n",
        "        'regressor__n_estimators': [100, 200, 300],\n",
        "        'regressor__max_depth': [None, 10, 20, 30],\n",
        "        'regressor__min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "    model_to_tune = pipeline_rf\n",
        "elif best_model_name == \"Gradient Boosting\":\n",
        "    param_grid = {\n",
        "        'regressor__n_estimators': [100, 200, 300],\n",
        "        'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
        "        'regressor__max_depth': [3, 5, 7]\n",
        "    }\n",
        "    model_to_tune = pipeline_gb\n",
        "elif best_model_name == \"XGBoost\":\n",
        "    param_grid = {\n",
        "        'regressor__n_estimators': [100, 200, 300],\n",
        "        'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
        "        'regressor__max_depth': [3, 5, 7]\n",
        "    }\n",
        "    model_to_tune = pipeline_xgb\n",
        "else:  # Linear Regression doesn't have much to tune\n",
        "    param_grid = {}\n",
        "    model_to_tune = pipeline_lr\n",
        "\n",
        "if param_grid:\n",
        "    grid_search = GridSearchCV(model_to_tune, param_grid, cv=5, scoring='r2')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Evaluate the tuned model\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Tuned Model Performance:\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"R²: {r2:.4f}\")\n",
        "else:\n",
        "    print(\"No hyperparameter tuning for Linear Regression.\")\n",
        "\n",
        "# Visualize predicted vs actual values\n",
        "plt.figure(figsize=(10, 6))\n",
        "y_pred = best_model.predict(X_test)\n",
        "plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
        "plt.xlabel('Actual Price (Lakhs)')\n",
        "plt.ylabel('Predicted Price (Lakhs)')\n",
        "plt.title('Actual vs Predicted Prices')\n",
        "plt.savefig('actual_vs_predicted.png')\n",
        "plt.close()\n",
        "\n",
        "# Feature importance for tree-based models\n",
        "if best_model_name in [\"Random Forest\", \"Gradient Boosting\", \"XGBoost\"]:\n",
        "    # Extract feature names from the preprocessor\n",
        "    cat_features = best_model.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(categorical_features)\n",
        "    feature_names = np.concatenate([numeric_features, cat_features])\n",
        "\n",
        "    # Extract feature importances\n",
        "    importances = best_model.named_steps['regressor'].feature_importances_\n",
        "\n",
        "    # Plot the feature importances\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    indices = np.argsort(importances)[-20:]  # Get indices of top 20 features\n",
        "    plt.barh(range(len(indices)), importances[indices])\n",
        "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.title('Top 20 Feature Importances')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance.png')\n",
        "    plt.close()\n",
        "\n",
        "# Residual plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "residuals = y_test - y_pred\n",
        "plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Price (Lakhs)')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.savefig('residual_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# Deep Learning Model (Simple Neural Network)\n",
        "print(\"\\n---- Training Deep Learning Model ----\")\n",
        "\n",
        "# Only proceed if TensorFlow is available\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "\n",
        "    # Preprocess the data\n",
        "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "    X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=[X_train_preprocessed.shape[1]]),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = keras.callbacks.EarlyStopping(\n",
        "        patience=20,\n",
        "        min_delta=0.001,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train_preprocessed, y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred_dl = model.predict(X_test_preprocessed).flatten()\n",
        "    dl_mse = mean_squared_error(y_test, y_pred_dl)\n",
        "    dl_rmse = np.sqrt(dl_mse)\n",
        "    dl_mae = mean_absolute_error(y_test, y_pred_dl)\n",
        "    dl_r2 = r2_score(y_test, y_pred_dl)\n",
        "\n",
        "    print(f\"\\nDeep Learning Model Performance:\")\n",
        "    print(f\"RMSE: {dl_rmse:.2f}\")\n",
        "    print(f\"MAE: {dl_mae:.2f}\")\n",
        "    print(f\"R²: {dl_r2:.4f}\")\n",
        "\n",
        "    # Plot learning curves\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss (MSE)')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['mae'])\n",
        "    plt.plot(history.history['val_mae'])\n",
        "    plt.title('Model MAE')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('deep_learning_curves.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Compare DL with best traditional ML model\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_test, y_pred_dl, alpha=0.5, label='Deep Learning')\n",
        "    plt.scatter(y_test, y_pred, alpha=0.5, label=best_model_name)\n",
        "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
        "    plt.xlabel('Actual Price (Lakhs)')\n",
        "    plt.ylabel('Predicted Price (Lakhs)')\n",
        "    plt.title('Traditional ML vs Deep Learning')\n",
        "    plt.legend()\n",
        "    plt.savefig('ml_vs_dl_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Save DL model if it's the best\n",
        "    if dl_r2 > results[0]['r2']:\n",
        "        print(\"Deep Learning model outperforms traditional ML models.\")\n",
        "        # Save the TF model\n",
        "        model.save('house_price_model_dl')\n",
        "        # Save the preprocessor\n",
        "        with open('preprocessor.pkl', 'wb') as f:\n",
        "            pickle.dump(preprocessor, f)\n",
        "        final_model_type = \"Deep Learning\"\n",
        "    else:\n",
        "        print(f\"Traditional ML model ({best_model_name}) outperforms Deep Learning.\")\n",
        "        final_model_type = best_model_name\n",
        "\n",
        "except ImportError:\n",
        "    print(\"TensorFlow not available. Skipping deep learning model.\")\n",
        "    final_model_type = best_model_name\n",
        "\n",
        "# Save the best model\n",
        "print(\"\\n---- Saving the Best Model ----\")\n",
        "\n",
        "# Save the best model (traditional ML)\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model, f)\n",
        "print(\"Model saved as 'model.pkl'\")\n",
        "\n",
        "# Save the preprocessor separately (useful for the API)\n",
        "with open('preprocessor.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessor, f)\n",
        "print(\"Preprocessor saved as 'preprocessor.pkl'\")\n",
        "\n",
        "# Create a metadata file with model information\n",
        "metadata = {\n",
        "    'model_type': final_model_type,\n",
        "    'features': list(X.columns),\n",
        "    'performance': {\n",
        "        'rmse': float(results[0]['rmse']),\n",
        "        'mae': float(results[0]['mae']),\n",
        "        'r2': float(results[0]['r2'])\n",
        "    }\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('model_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "print(\"Model metadata saved as 'model_metadata.json'\")\n",
        "\n",
        "print(\"\\n---- Model Building Complete ----\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6c2a7e",
      "metadata": {
        "id": "bb6c2a7e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}